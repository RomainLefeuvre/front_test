{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet File Analysis for DuckDB-WASM Optimization\n",
    "\n",
    "This notebook analyzes parquet files to check:\n",
    "- Bloom filters presence\n",
    "- Statistics (min/max) quality\n",
    "- Row group sizes\n",
    "- Sorting order\n",
    "- Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to parquet files\n",
    "PARQUET_DIR = Path(\"../input_data/vulnerable_origins\")\n",
    "\n",
    "# Get first few files for analysis\n",
    "parquet_files = sorted(PARQUET_DIR.glob(\"*.parquet\"))[:5]\n",
    "print(f\"Found {len(list(PARQUET_DIR.glob('*.parquet')))} total parquet files\")\n",
    "print(f\"Analyzing first {len(parquet_files)} files:\")\n",
    "for f in parquet_files:\n",
    "    size_mb = os.path.getsize(f) / 1024 / 1024\n",
    "    print(f\"  - {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic File Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file_basic(filepath):\n",
    "    \"\"\"Get basic file information\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    return {\n",
    "        'file': filepath.name,\n",
    "        'size_mb': os.path.getsize(filepath) / 1024 / 1024,\n",
    "        'num_rows': meta.num_rows,\n",
    "        'num_row_groups': meta.num_row_groups,\n",
    "        'num_columns': meta.num_columns,\n",
    "        'created_by': meta.created_by,\n",
    "        'format_version': meta.format_version,\n",
    "    }\n",
    "\n",
    "for f in parquet_files:\n",
    "    info = analyze_file_basic(f)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"File: {info['file']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Size: {info['size_mb']:.2f} MB\")\n",
    "    print(f\"Rows: {info['num_rows']:,}\")\n",
    "    print(f\"Row Groups: {info['num_row_groups']}\")\n",
    "    print(f\"Columns: {info['num_columns']}\")\n",
    "    print(f\"Created by: {info['created_by']}\")\n",
    "    print(f\"Format version: {info['format_version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze schema of first file\n",
    "pf = pq.ParquetFile(parquet_files[0])\n",
    "print(\"Schema:\")\n",
    "print(pf.schema_arrow)\n",
    "print(\"\\nParquet Schema:\")\n",
    "print(pf.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bloom Filter Check\n",
    "\n",
    "Bloom filters allow DuckDB to quickly skip row groups that definitely don't contain a value.\n",
    "Without bloom filters, DuckDB must download and scan row groups to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bloom_filters(filepath):\n",
    "    \"\"\"Check if bloom filters exist in the parquet file\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Check first few row groups\n",
    "    for rg_idx in range(min(3, meta.num_row_groups)):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        \n",
    "        for col_idx in range(meta.num_columns):\n",
    "            col = rg.column(col_idx)\n",
    "            \n",
    "            # Try to detect bloom filter\n",
    "            # PyArrow doesn't expose bloom_filter_offset directly in newer versions\n",
    "            # We check via the column metadata\n",
    "            col_dict = col.to_dict()\n",
    "            \n",
    "            has_bloom = 'bloom_filter_offset' in col_dict and col_dict.get('bloom_filter_offset') is not None\n",
    "            \n",
    "            if rg_idx == 0:  # Only report once per column\n",
    "                results.append({\n",
    "                    'column': col.path_in_schema,\n",
    "                    'has_bloom_filter': has_bloom,\n",
    "                    'bloom_filter_offset': col_dict.get('bloom_filter_offset'),\n",
    "                    'bloom_filter_length': col_dict.get('bloom_filter_length'),\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Bloom Filter Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for f in parquet_files[:1]:  # Check first file\n",
    "    print(f\"\\nFile: {f.name}\")\n",
    "    results = check_bloom_filters(f)\n",
    "    \n",
    "    has_any_bloom = any(r['has_bloom_filter'] for r in results)\n",
    "    \n",
    "    if has_any_bloom:\n",
    "        print(\"✓ Bloom filters FOUND\")\n",
    "    else:\n",
    "        print(\"✗ NO bloom filters - queries will be slow!\")\n",
    "    \n",
    "    print(\"\\nPer-column details:\")\n",
    "    for r in results:\n",
    "        status = \"✓\" if r['has_bloom_filter'] else \"✗\"\n",
    "        print(f\"  {status} {r['column']}: bloom_offset={r['bloom_filter_offset']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistics Analysis\n",
    "\n",
    "Min/max statistics allow DuckDB to skip row groups where the searched value is outside the range.\n",
    "For this to work well, data should be SORTED by the query column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_statistics(filepath, column_name='origin', num_row_groups=10):\n",
    "    \"\"\"Analyze statistics for a specific column across row groups\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    # Find column index\n",
    "    col_idx = None\n",
    "    for i in range(meta.num_columns):\n",
    "        if meta.row_group(0).column(i).path_in_schema == column_name:\n",
    "            col_idx = i\n",
    "            break\n",
    "    \n",
    "    if col_idx is None:\n",
    "        print(f\"Column '{column_name}' not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Statistics for column '{column_name}':\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    stats_list = []\n",
    "    for rg_idx in range(min(num_row_groups, meta.num_row_groups)):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        col = rg.column(col_idx)\n",
    "        stats = col.statistics\n",
    "        \n",
    "        if stats and stats.has_min_max:\n",
    "            min_val = str(stats.min)[:50]\n",
    "            max_val = str(stats.max)[:50]\n",
    "            stats_list.append({'min': stats.min, 'max': stats.max})\n",
    "            print(f\"RG {rg_idx:3d}: min='{min_val}' | max='{max_val}'\")\n",
    "        else:\n",
    "            print(f\"RG {rg_idx:3d}: NO STATISTICS\")\n",
    "    \n",
    "    # Check if data appears sorted\n",
    "    if len(stats_list) > 1:\n",
    "        is_sorted = all(\n",
    "            stats_list[i]['max'] <= stats_list[i+1]['min'] \n",
    "            for i in range(len(stats_list)-1)\n",
    "        )\n",
    "        \n",
    "        # Check overlap\n",
    "        overlaps = sum(\n",
    "            1 for i in range(len(stats_list)-1)\n",
    "            if stats_list[i]['max'] > stats_list[i+1]['min']\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        if is_sorted:\n",
    "            print(\"✓ Data appears SORTED - statistics will be effective!\")\n",
    "        else:\n",
    "            print(f\"✗ Data NOT sorted - {overlaps} overlapping row groups\")\n",
    "            print(\"  → Most row groups will need to be downloaded\")\n",
    "\n",
    "# Analyze first file\n",
    "analyze_statistics(parquet_files[0], 'origin', num_row_groups=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Row Group Size Analysis\n",
    "\n",
    "Smaller row groups = more granular filtering, but more metadata overhead.\n",
    "Recommended: 100K-500K rows per row group for browser queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_row_groups(filepath):\n",
    "    \"\"\"Analyze row group sizes\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    sizes = []\n",
    "    rows = []\n",
    "    \n",
    "    for rg_idx in range(meta.num_row_groups):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        sizes.append(rg.total_byte_size)\n",
    "        rows.append(rg.num_rows)\n",
    "    \n",
    "    avg_size_mb = sum(sizes) / len(sizes) / 1024 / 1024\n",
    "    avg_rows = sum(rows) / len(rows)\n",
    "    min_size_mb = min(sizes) / 1024 / 1024\n",
    "    max_size_mb = max(sizes) / 1024 / 1024\n",
    "    \n",
    "    print(f\"Row Group Analysis for {filepath.name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total row groups: {len(sizes)}\")\n",
    "    print(f\"Avg rows per group: {avg_rows:,.0f}\")\n",
    "    print(f\"Avg size per group: {avg_size_mb:.2f} MB\")\n",
    "    print(f\"Min size: {min_size_mb:.2f} MB\")\n",
    "    print(f\"Max size: {max_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\nRecommendation:\")\n",
    "    if avg_rows > 500_000:\n",
    "        print(f\"  ⚠ Row groups are large ({avg_rows:,.0f} rows)\")\n",
    "        print(\"  → Consider smaller row groups (100K-500K) for finer filtering\")\n",
    "    elif avg_rows < 50_000:\n",
    "        print(f\"  ⚠ Row groups are small ({avg_rows:,.0f} rows)\")\n",
    "        print(\"  → May have too much metadata overhead\")\n",
    "    else:\n",
    "        print(f\"  ✓ Row group size looks good ({avg_rows:,.0f} rows)\")\n",
    "\n",
    "for f in parquet_files[:2]:\n",
    "    analyze_row_groups(f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compression(filepath):\n",
    "    \"\"\"Analyze compression settings\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    rg = meta.row_group(0)\n",
    "    \n",
    "    print(f\"Compression Analysis for {filepath.name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for col_idx in range(meta.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        compressed = col.total_compressed_size\n",
    "        uncompressed = col.total_uncompressed_size\n",
    "        ratio = uncompressed / compressed if compressed > 0 else 0\n",
    "        \n",
    "        print(f\"{col.path_in_schema}:\")\n",
    "        print(f\"  Compression: {col.compression}\")\n",
    "        print(f\"  Encodings: {col.encodings}\")\n",
    "        print(f\"  Ratio: {ratio:.1f}x ({compressed/1024:.1f} KB → {uncompressed/1024:.1f} KB)\")\n",
    "        print()\n",
    "\n",
    "analyze_compression(parquet_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_analysis(filepath):\n",
    "    \"\"\"Complete analysis with recommendations\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check file size\n",
    "    size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "    if size_mb > 500:\n",
    "        issues.append(f\"File too large ({size_mb:.0f} MB) - browser may run out of memory\")\n",
    "    \n",
    "    # Check bloom filters\n",
    "    rg = meta.row_group(0)\n",
    "    col = rg.column(0)  # Check first column (origin)\n",
    "    col_dict = col.to_dict()\n",
    "    if not col_dict.get('bloom_filter_offset'):\n",
    "        issues.append(\"No bloom filters - queries will download unnecessary data\")\n",
    "    \n",
    "    # Check sorting\n",
    "    stats_list = []\n",
    "    for rg_idx in range(min(10, meta.num_row_groups)):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        col = rg.column(0)\n",
    "        if col.statistics and col.statistics.has_min_max:\n",
    "            stats_list.append({'min': col.statistics.min, 'max': col.statistics.max})\n",
    "    \n",
    "    if len(stats_list) > 1:\n",
    "        overlaps = sum(\n",
    "            1 for i in range(len(stats_list)-1)\n",
    "            if stats_list[i]['max'] > stats_list[i+1]['min']\n",
    "        )\n",
    "        if overlaps > 0:\n",
    "            issues.append(f\"Data not sorted by 'origin' - {overlaps} overlapping row groups\")\n",
    "    \n",
    "    # Check row group size\n",
    "    avg_rows = meta.num_rows / meta.num_row_groups\n",
    "    if avg_rows > 500_000:\n",
    "        issues.append(f\"Row groups too large ({avg_rows:,.0f} rows) - use 100K-500K\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PARQUET OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_issues = []\n",
    "for f in parquet_files:\n",
    "    issues = full_analysis(f)\n",
    "    all_issues.extend(issues)\n",
    "    \n",
    "    print(f\"\\n{f.name}:\")\n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            print(f\"  ✗ {issue}\")\n",
    "    else:\n",
    "        print(\"  ✓ All checks passed!\")\n",
    "\n",
    "if all_issues:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDED ACTIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "Run the optimization script to fix these issues:\n",
    "\n",
    "    python scripts/optimizeParquet.py input_data/vulnerable_origins output_data/vulnerable_origins\n",
    "\n",
    "This will:\n",
    "1. Sort data by 'origin' for better statistics pruning\n",
    "2. Add bloom filters for fast negative lookups  \n",
    "3. Use smaller row groups (100K rows) for finer filtering\n",
    "4. Split large files to avoid browser memory issues\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Query Simulation\n",
    "\n",
    "Simulate how many row groups would need to be downloaded for a specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_query(filepath, search_value='https://github.com/django/django'):\n",
    "    \"\"\"Simulate how many row groups would be downloaded for a query\"\"\"\n",
    "    pf = pq.ParquetFile(filepath)\n",
    "    meta = pf.metadata\n",
    "    \n",
    "    # Find origin column\n",
    "    col_idx = 0  # Assuming origin is first column\n",
    "    \n",
    "    would_download = 0\n",
    "    would_skip_stats = 0\n",
    "    would_skip_bloom = 0\n",
    "    \n",
    "    total_bytes_download = 0\n",
    "    total_bytes_skip = 0\n",
    "    \n",
    "    for rg_idx in range(meta.num_row_groups):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        col = rg.column(col_idx)\n",
    "        stats = col.statistics\n",
    "        \n",
    "        # Check if statistics would skip this row group\n",
    "        if stats and stats.has_min_max:\n",
    "            if search_value < stats.min or search_value > stats.max:\n",
    "                would_skip_stats += 1\n",
    "                total_bytes_skip += rg.total_byte_size\n",
    "                continue\n",
    "        \n",
    "        # If we get here, we'd need to download (bloom filter would help but we can't check it here)\n",
    "        would_download += 1\n",
    "        total_bytes_download += rg.total_byte_size\n",
    "    \n",
    "    print(f\"Query Simulation: WHERE origin = '{search_value}'\")\n",
    "    print(f\"File: {filepath.name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total row groups: {meta.num_row_groups}\")\n",
    "    print(f\"Would skip (via stats): {would_skip_stats} ({would_skip_stats/meta.num_row_groups*100:.1f}%)\")\n",
    "    print(f\"Would download: {would_download} ({would_download/meta.num_row_groups*100:.1f}%)\")\n",
    "    print(f\"\\nData transfer:\")\n",
    "    print(f\"  Would download: {total_bytes_download/1024/1024:.1f} MB\")\n",
    "    print(f\"  Would skip: {total_bytes_skip/1024/1024:.1f} MB\")\n",
    "    \n",
    "    if would_download > meta.num_row_groups * 0.1:\n",
    "        print(f\"\\n⚠ WARNING: Would download {would_download/meta.num_row_groups*100:.0f}% of row groups!\")\n",
    "        print(\"  → Data needs to be sorted by 'origin' for efficient queries\")\n",
    "\n",
    "# Test with a sample query\n",
    "simulate_query(parquet_files[0], 'https://github.com/django/django')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
